import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset, concatenate_datasets
import numpy as np
from PIL import Image
import torchvision.transforms as T
import matplotlib.pyplot as plt

# ===============================
# Tokenizer funcional
# ===============================
class SimpleTokenizer:
    def __init__(self, vocab_size=50000):
        self.vocab = {f"TOKEN_{i}": i for i in range(vocab_size)}
        self.vocab["[PAD]"] = 0
        self.vocab["[UNK]"] = 1
        self.vocab_size = len(self.vocab)

    def encode(self, text):
        return [self.vocab.get(tok, self.vocab["[UNK]"]) for tok in text.split()]

    def decode(self, ids):
        inv_vocab = {v:k for k,v in self.vocab.items()}
        return " ".join([inv_vocab.get(i,"[UNK]") for i in ids])

# ===============================
# Transformer Block
# ===============================
class AdvancedTransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_hidden):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_hidden),
            nn.GELU(),
            nn.Linear(ff_hidden, embed_dim)
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_out,_ = self.attn(x, x, x)
        x = self.norm1(x + attn_out)
        x = self.norm2(x + self.ff(x))
        return x

# ===============================
# VAE para imagens alta resolução
# ===============================
class ImageVAE(nn.Module):
    def __init__(self, dim=2048):
        super().__init__()
        # Encoder reduz resolução
        self.encoder = nn.Sequential(
            nn.Conv2d(3, dim, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim, dim*2, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*2, dim*4, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*4, dim*8, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*8, dim, 4, 2, 1)
        )
        # Decoder reconstrói a imagem
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(dim, dim*8, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*8, dim*4, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*4, dim*2, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*2, dim, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim, 3, 4, 2, 1), nn.Sigmoid()
        )

    def encode(self, x):
        return self.encoder(x)

    def decode(self, z):
        return self.decoder(z)

    def show_image(self, tensor):
        img = tensor.detach().cpu().clamp(0,1).permute(1,2,0).numpy()
        plt.figure(figsize=(12,12))
        plt.imshow(img)
        plt.axis('off')
        plt.show()

# ===============================
# Dataset multimodal completo
# ===============================
class GPTMiniOmniDataset(Dataset):
    def __init__(self, tokenizer, max_len=512):
        self.tokenizer = tokenizer
        self.max_len = max_len

        # Texto: Wikipedia
        langs = ["en","pt","fr","es","de"]
        wiki_datasets = [load_dataset("wikipedia", f"20220301.{lang}", split="train") for lang in langs]
        text_dataset = concatenate_datasets(wiki_datasets)

        # Código: The Stack
        code_langs = ["python","javascript","java","cpp","c"]
        code_datasets = [load_dataset("bigcode/the-stack-dedup", data_dir=f"data/{lang}", split="train") for lang in code_langs]
        code_dataset = concatenate_datasets(code_datasets)

        # Imagens: COCO completo
        image_dataset = load_dataset("coco", "2017", split="train")

        # Raciocínio: GSM8K + BigBench logical_deduction
        try:
            reasoning_datasets = [
                load_dataset("gsm8k", split="train"),
                load_dataset("bigbench", "logical_deduction", split="train")
            ]
            reasoning_dataset = concatenate_datasets(reasoning_datasets)
        except:
            reasoning_dataset = text_dataset

        self.text_dataset = text_dataset
        self.code_dataset = code_dataset
        self.image_dataset = image_dataset
        self.reasoning_dataset = reasoning_dataset

        self.transform_image = T.Compose([
            T.Resize((256,256)),
            T.ToTensor()
        ])

        self.num_audio = 10000

    def __len__(self):
        return max(len(self.text_dataset), len(self.code_dataset),
                   len(self.image_dataset), len(self.reasoning_dataset),
                   self.num_audio)

    def __getitem__(self, idx):
        # Texto
        text_sample = self.text_dataset[idx % len(self.text_dataset)]["text"]
        text_tokens = torch.tensor(self.tokenizer.encode(text_sample)[:self.max_len], dtype=torch.long)

        # Código
        code_sample = self.code_dataset[idx % len(self.code_dataset)]["code"]
        code_tokens = torch.tensor(self.tokenizer.encode(code_sample)[:self.max_len], dtype=torch.long)

        # Imagem
        img = self.image_dataset[idx % len(self.image_dataset)]["image"]
        if isinstance(img, np.ndarray):
            img = Image.fromarray(img)
        image_tensor = self.transform_image(img)

        # Raciocínio
        reasoning_sample = self.reasoning_dataset[idx % len(self.reasoning_dataset)]
        reasoning_text = reasoning_sample.get("question", reasoning_sample.get("text", ""))
        reasoning_tokens = torch.tensor(self.tokenizer.encode(reasoning_text)[:self.max_len], dtype=torch.long)

        # Áudio placeholder
        audio_tensor = torch.rand((256,1024))

        return text_tokens, code_tokens, image_tensor, reasoning_tokens, audio_tensor

# ===============================
# GPT-Mini-4-Omni grande
# ===============================
class GPTMini4Omni(nn.Module):
    def __init__(self, vocab_size=50000, embed_dim=2048, depth=16, num_heads=20):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_embedding = nn.Parameter(torch.randn(1, 512, embed_dim))
        self.transformer_blocks = nn.ModuleList([AdvancedTransformerBlock(embed_dim, num_heads, embed_dim*4) for _ in range(depth)])
        self.ln_f = nn.LayerNorm(embed_dim)
        self.text_head = nn.Linear(embed_dim, vocab_size)
        self.code_head = nn.Linear(embed_dim, vocab_size)
        self.reasoning_head = nn.Linear(embed_dim, vocab_size)
        self.image_vae = ImageVAE(dim=embed_dim//2)
        self.audio_fc = nn.Linear(1024, embed_dim)

    def forward_text(self, input_ids):
        x = self.token_embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1)]
        for block in self.transformer_blocks:
            x = block(x)
        x = self.ln_f(x)
        return self.text_head(x)

    def forward_code(self, input_ids):
        x = self.token_embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1)]
        for block in self.transformer_blocks:
            x = block(x)
        x = self.ln_f(x)
        return self.code_head(x)

    def forward_reasoning(self, input_ids):
        x = self.token_embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1)]
        for block in self.transformer_blocks:
            x = block(x)
        x = self.ln_f(x)
        return self.reasoning_head(x)

    def forward_image(self, image_tensor):
        latent = self.image_vae.encode(image_tensor.unsqueeze(0))
        return self.image_vae.decode(latent)[0]

    def forward_audio(self, audio_tensor):
        return self.audio_fc(audio_tensor.mean(dim=0, keepdim=True))

# ===============================
# Geração de texto
# ===============================
def generate_text(model, tokenizer, prompt, max_len=50):
    model.eval()
    ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0)
    for _ in range(max_len):
        logits = model.forward_text(ids)
        next_token = torch.argmax(logits[:, -1, :], dim=-1)
        ids = torch.cat([ids, next_token.unsqueeze(0)], dim=1)
    return tokenizer.decode(ids[0].tolist())

# ===============================
# Treinamento multimodal real
# ===============================
def train_real(epochs=1, batch_size=2):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = SimpleTokenizer()
    dataset = GPTMiniOmniDataset(tokenizer)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    model = GPTMini4Omni().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion_text = nn.CrossEntropyLoss()
    criterion_code = nn.CrossEntropyLoss()
    criterion_reasoning = nn.CrossEntropyLoss()
    criterion_image = nn.MSELoss()
    criterion_audio = nn.MSELoss()

    model.train()
    for epoch in range(epochs):
        for batch_idx, (text_tokens, code_tokens, image_tensor, reasoning_tokens, audio_tensor) in enumerate(dataloader):
            text_tokens, code_tokens = text_tokens.to(device), code_tokens.to(device)
            image_tensor, audio_tensor = image_tensor.to(device), audio_tensor.to(device)
            reasoning_tokens = reasoning_tokens.to(device)

            optimizer.zero_grad()
            # Texto
            logits_text = model.forward_text(text_tokens)
            loss_text = criterion_text(logits_text.view(-1, logits_text.size(-1)), text_tokens.view(-1))
            # Código
            logits_code = model.forward_code(code_tokens)
            loss_code = criterion_code(logits_code.view(-1, logits_code.size(-1)), code_tokens.view(-1))
            # Raciocínio
            logits_reasoning = model.forward_reasoning(reasoning_tokens)
            loss_reasoning = criterion_reasoning(logits_reasoning.view(-1, logits_reasoning.size(-1)), reasoning_tokens.view(-1))
            # Imagem
            recon_img = model.forward_image(image_tensor)
            loss_image = criterion_image(recon_img, image_tensor)
            # Áudio
            pred_audio = model.forward_audio(audio_tensor)
            loss_audio = criterion_audio(pred_audio, audio_tensor.mean(dim=0, keepdim=True))

            loss = loss_text + loss_code + loss_reasoning + loss_image + loss_audio
            loss.backward()
            optimizer.step()

            if batch_idx % 5 == 0:
                print(f"Epoch {epoch+1} Batch {batch_idx} | "
                      f"Text: {loss_text.item():.4f} | "
                      f"Code: {loss_code.item():.4f} | "
                      f"Reasoning: {loss_reasoning.item():.4f} | "
                      f"Image: {loss_image.item():.4f} | "
                      f"Audio: {loss_audio.item():.4f}")

    return model, tokenizer

# ===============================
# Exemplo de uso
# ===============================
if __name__ == "__main__":
    model, tokenizer = train_real(epochs=1, batch_size=2)
    print(generate_text(model, tokenizer, "Once upon a time"))
