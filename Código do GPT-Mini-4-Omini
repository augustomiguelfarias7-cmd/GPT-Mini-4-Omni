import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset, concatenate_datasets
import numpy as np
from PIL import Image  # Para visualizar imagens
import matplotlib.pyplot as plt  # Para exibir imagens em notebooks ou apps
import os

# ============================================
# Tokenizer multil√≠ngue completo
# ============================================
class SimpleTokenizer:
    def __init__(self, vocab_size=50000):
        # Vocabul√°rio simples, substitui palavras desconhecidas por [UNK]
        self.vocab = {f"TOKEN_{i}": i for i in range(vocab_size)}
        self.vocab["[PAD]"] = 0
        self.vocab["[UNK]"] = 1
        self.vocab_size = len(self.vocab)

    def encode(self, text):
        # Divide por espa√ßos e converte em ids
        return [self.vocab.get(tok, self.vocab["[UNK]"]) for tok in text.split()]

    def decode(self, ids):
        inv_vocab = {v:k for k,v in self.vocab.items()}
        return " ".join([inv_vocab.get(i,"[UNK]") for i in ids])

# ============================================
# Transformer Block avan√ßado
# ============================================
class AdvancedTransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_hidden):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_hidden),
            nn.GELU(),
            nn.Linear(ff_hidden, embed_dim)
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_out,_ = self.attn(x, x, x)
        x = self.norm1(x + attn_out)
        x = self.norm2(x + self.ff(x))
        return x

# ============================================
# VAE para imagens 12K (com tiling e pixel progressivo)
# ============================================
class ImageVAE(nn.Module):
    def __init__(self, dim=2048, tile_size=1024):
        super().__init__()
        self.tile_size = tile_size
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, dim, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim, dim*2, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*2, dim*4, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*4, dim*8, 4, 2, 1), nn.ReLU()
        )
        self.to_latent = nn.Conv2d(dim*8, dim, 1)
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(dim, dim*8, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*8, dim*4, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*4, dim*2, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*2, dim, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim, 3, 4, 2, 1)
        )

    def encode(self, x):
        return self.to_latent(self.encoder(x))

    def decode(self, z):
        return self.decoder(z)

    # Gera√ß√£o progressiva de imagem 12K usando tiles
    def generate_12k(self, device, show_progress=False):
        full_size = 12288
        n_tiles = full_size // self.tile_size
        canvas = torch.zeros((3, full_size, full_size), device=device)
        for i in range(n_tiles):
            for j in range(n_tiles):
                # Gera√ß√£o inicial em resolu√ß√£o menor
                tile = torch.rand((1, 3, self.tile_size, self.tile_size), device=device)
                latent = self.encode(tile)
                recon = self.decode(latent)
                canvas[:, i*self.tile_size:(i+1)*self.tile_size,
                          j*self.tile_size:(j+1)*self.tile_size] = recon[0]
                if show_progress:
                    print(f"Tile {i},{j} gerado")
        return canvas

    # Fun√ß√£o para exibir imagem no PIL/matplotlib
    def show_image(self, tensor):
        img = tensor.detach().cpu().clamp(0,1).permute(1,2,0).numpy()  # (H,W,C)
        plt.figure(figsize=(12,12))
        plt.imshow(img)
        plt.axis('off')
        plt.show()

# ============================================
# Dataset multimodal completo
# ============================================
class GPTMiniOmniDataset(Dataset):
    def __init__(self, tokenizer, max_len=512):
        self.tokenizer = tokenizer
        self.max_len = max_len

        # Wikipedia em v√°rios idiomas
        langs = ["pt","en","fr","es","de","it","ru","zh","ja","ko"]
        wiki_datasets = [load_dataset("wikipedia", f"20220301.{lang}", split="train") for lang in langs]
        self.text_dataset = concatenate_datasets(wiki_datasets)

        # Dataset de c√≥digo completo
        code_langs = ["python","javascript","java","cpp","c"]
        code_datasets = [load_dataset("bigcode/the-stack-dedup", data_dir=f"data/{lang}", split="train") for lang in code_langs]
        self.code_dataset = concatenate_datasets(code_datasets)

        # Placeholders multimodal
        self.num_images = 1000
        self.num_videos = 200
        self.num_audio = 500

    def __len__(self):
        return max(len(self.text_dataset), len(self.code_dataset), self.num_images, self.num_videos, self.num_audio)

    def __getitem__(self, idx):
        # Texto
        text_sample = self.text_dataset[idx % len(self.text_dataset)]["text"]
        text_tokens = torch.tensor(self.tokenizer.encode(text_sample)[:self.max_len], dtype=torch.long)
        # C√≥digo
        code_sample = self.code_dataset[idx % len(self.code_dataset)]["code"]
        code_tokens = torch.tensor(self.tokenizer.encode(code_sample)[:self.max_len], dtype=torch.long)
        # Imagem, v√≠deo e √°udio (placeholder)
        image_tensor = torch.rand((3, 1024, 1024))
        video_tensor = torch.rand((8, 3, 256, 256))
        audio_tensor = torch.rand((256, 1024))
        return text_tokens, code_tokens, image_tensor, video_tensor, audio_tensor

# ============================================
# GPT-Mini-4-Omni (multimodal, trein√°vel)
# ============================================
class GPTMini4Omni(nn.Module):
    def __init__(self, vocab_size=50000, embed_dim=2048, depth=24, num_heads=16):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_embedding = nn.Parameter(torch.randn(1, 512, embed_dim))
        self.transformer_blocks = nn.ModuleList([AdvancedTransformerBlock(embed_dim, num_heads, embed_dim*4) for _ in range(depth)])
        self.ln_f = nn.LayerNorm(embed_dim)
        self.text_head = nn.Linear(embed_dim, vocab_size)
        self.image_vae = ImageVAE(dim=embed_dim//2)
        self.audio_fc = nn.Linear(1024, embed_dim)

    # Forward multimodal
    def forward_text(self, input_ids):
        x = self.token_embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1)]
        for block in self.transformer_blocks:
            x = block(x)
        x = self.ln_f(x)
        return self.text_head(x)

    def forward_image(self, image_tensor):
        latent = self.image_vae.encode(image_tensor)
        return self.image_vae.decode(latent)

    def forward_audio(self, audio_tensor):
        return self.audio_fc(audio_tensor.mean(dim=0, keepdim=True))

    def forward_video(self, video_tensor):
        B, F, C, H, W = video_tensor.shape
        video_tensor = video_tensor.view(B*F, C, H, W)
        processed = self.forward_image(video_tensor)
        return processed.view(B, F, C, H, W)

    # Sentimento
    def analyze_user_sentiment(self, text_input):
        if any(word in text_input.lower() for word in ["triste", "depress", "chateado", "angry"]):
            return "üò¢ Parece que voc√™ est√° triste. Que tal um conte√∫do motivacional?"
        return "üôÇ Tudo certo!"

# ============================================
# Treinamento completo multimodal 100%
# ============================================
def train_full(epochs=1, batch_size=1, show_progress=False):
    tokenizer = SimpleTokenizer()
    dataset = GPTMiniOmniDataset(tokenizer)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = GPTMini4Omni().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    scaler = torch.cuda.amp.GradScaler()

    for epoch in range(epochs):
        for batch_idx, (text_tokens, code_tokens, image_tensor, video_tensor, audio_tensor) in enumerate(dataloader):
            text_tokens, code_tokens = text_tokens.to(device), code_tokens.to(device)
            image_tensor, video_tensor = image_tensor.to(device), video_tensor.to(device)
            audio_tensor = audio_tensor.to(device)

            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                text_logits = model.forward_text(text_tokens)
                text_loss = criterion(text_logits.float(), torch.randn_like(text_logits))

                image_out = model.forward_image(image_tensor)
                image_loss = criterion(image_out, image_tensor)

                video_out = model.forward_video(video_tensor)
                video_loss = criterion(video_out, video_tensor)

                audio_out = model.forward_audio(audio_tensor)
                audio_loss = criterion(audio_out, torch.randn_like(audio_out))

                total_loss = text_loss + image_loss + video_loss + audio_loss

            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()

            if show_progress and batch_idx % 10 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {total_loss.item():.4f}")

    print("‚úÖ Treinamento finalizado!")
    return model

# ============================================
# Exemplo de uso
# ============================================
if __name__ == "__main__":
    model = train_full(epochs=1, batch_size=1, show_progress=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    generated_12k = model.image_vae.generate_12k(device, show_progress=True)
    print("‚úÖ Imagem 12K gerada com shape:", generated_12k.shape)
    # Mostrar a imagem gerada
    model.image_vae.show_image(generated_12k)

    msg = "Estou muito chateado hoje"
    print(model.analyze_user_sentiment(msg))
