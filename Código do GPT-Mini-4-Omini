import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset, concatenate_datasets
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as T
import os

# ============================================
# Tokenizer multil√≠ngue completo (80k tokens)
# ============================================
class SimpleTokenizer:
    def __init__(self, vocab_size=80000):
        self.vocab = {f"TOKEN_{i}": i for i in range(vocab_size)}
        self.vocab["[PAD]"] = 0
        self.vocab["[UNK]"] = 1
        self.vocab_size = len(self.vocab)

    def encode(self, text):
        return [self.vocab.get(tok, self.vocab["[UNK]"]) for tok in text.split()]

    def decode(self, ids):
        inv_vocab = {v:k for k,v in self.vocab.items()}
        return " ".join([inv_vocab.get(i,"[UNK]") for i in ids])

# ============================================
# Transformer Block avan√ßado
# ============================================
class AdvancedTransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_hidden):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_hidden),
            nn.GELU(),
            nn.Linear(ff_hidden, embed_dim)
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x):
        attn_out,_ = self.attn(x, x, x)
        x = self.norm1(x + attn_out)
        x = self.norm2(x + self.ff(x))
        return x

# ============================================
# VAE para imagens
# ============================================
class ImageVAE(nn.Module):
    def __init__(self, dim=2048, tile_size=1024):
        super().__init__()
        self.tile_size = tile_size
        self.encoder = nn.Sequential(
            nn.Conv2d(3, dim, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim, dim*2, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*2, dim*4, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(dim*4, dim*8, 4, 2, 1), nn.ReLU()
        )
        self.to_latent = nn.Conv2d(dim*8, dim, 1)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(dim, dim*8, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*8, dim*4, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*4, dim*2, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim*2, dim, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(dim, 3, 4, 2, 1)
        )

    def encode(self, x):
        return self.to_latent(self.encoder(x))

    def decode(self, z):
        return self.decoder(z)

    def show_image(self, tensor):
        img = tensor.detach().cpu().clamp(0,1).permute(1,2,0).numpy()
        plt.figure(figsize=(12,12))
        plt.imshow(img)
        plt.axis('off')
        plt.show()

# ============================================
# Dataset multimodal real
# ============================================
class GPTMiniOmniDataset(Dataset):
    def __init__(self, tokenizer, max_len=512):
        self.tokenizer = tokenizer
        self.max_len = max_len

        # Texto
        langs = ["pt","en","fr","es","de","it","ru","zh","ja","ko"]
        wiki_datasets = [load_dataset("wikipedia", f"20220301.{lang}", split="train") for lang in langs]
        self.text_dataset = concatenate_datasets(wiki_datasets)

        # C√≥digo
        code_langs = ["python","javascript","java","cpp","c"]
        code_datasets = [load_dataset("bigcode/the-stack-dedup", data_dir=f"data/{lang}", split="train") for lang in code_langs]
        self.code_dataset = concatenate_datasets(code_datasets)

        # Imagens reais
        self.image_dataset = load_dataset("coco", "2017", split="train")  # COCO 2017
        self.transform_image = T.Compose([
            T.Resize((1024,1024)),
            T.ToTensor()
        ])

        # V√≠deos reais
        self.video_dataset = load_dataset("ucf101", split="train")  # UCF101
        self.transform_video = T.Compose([
            T.Resize((256,256)),
            T.ToTensor()
        ])

        # √Åudio placeholders
        self.num_audio = 500

    def __len__(self):
        return max(len(self.text_dataset), len(self.code_dataset), len(self.image_dataset), len(self.video_dataset), self.num_audio)

    def __getitem__(self, idx):
        # Texto
        text_sample = self.text_dataset[idx % len(self.text_dataset)]["text"]
        text_tokens = torch.tensor(self.tokenizer.encode(text_sample)[:self.max_len], dtype=torch.long)

        # C√≥digo
        code_sample = self.code_dataset[idx % len(self.code_dataset)]["code"]
        code_tokens = torch.tensor(self.tokenizer.encode(code_sample)[:self.max_len], dtype=torch.long)

        # Imagem
        img = self.image_dataset[idx % len(self.image_dataset)]["image"]
        if isinstance(img, np.ndarray):
            img = Image.fromarray(img)
        image_tensor = self.transform_image(img)

        # V√≠deo
        video_data = self.video_dataset[idx % len(self.video_dataset)]["video"]
        frames = torch.stack([self.transform_video(Image.fromarray(frame)) for frame in video_data["frames"][:8]])
        video_tensor = frames  # 8 frames por v√≠deo

        # √Åudio placeholder
        audio_tensor = torch.rand((256, 1024))

        return text_tokens, code_tokens, image_tensor, video_tensor, audio_tensor

# ============================================
# GPT-Mini-4-Omni
# ============================================
class GPTMini4Omni(nn.Module):
    def __init__(self, vocab_size=80000, embed_dim=2048, depth=24, num_heads=16):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_embedding = nn.Parameter(torch.randn(1, 512, embed_dim))
        self.transformer_blocks = nn.ModuleList([AdvancedTransformerBlock(embed_dim, num_heads, embed_dim*4) for _ in range(depth)])
        self.ln_f = nn.LayerNorm(embed_dim)
        self.text_head = nn.Linear(embed_dim, vocab_size)
        self.image_vae = ImageVAE(dim=embed_dim//2)
        self.audio_fc = nn.Linear(1024, embed_dim)

    def forward_text(self, input_ids):
        x = self.token_embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1)]
        for block in self.transformer_blocks:
            x = block(x)
        x = self.ln_f(x)
        return self.text_head(x)

    def forward_image(self, image_tensor):
        latent = self.image_vae.encode(image_tensor.unsqueeze(0))
        return self.image_vae.decode(latent)[0]

    def forward_audio(self, audio_tensor):
        return self.audio_fc(audio_tensor.mean(dim=0, keepdim=True))

    def forward_video(self, video_tensor):
        B, F, C, H, W = video_tensor.shape
        video_tensor = video_tensor.view(B*F, C, H, W)
        processed = self.forward_image(video_tensor)
        return processed.view(B, F, C, H, W)

    def analyze_user_sentiment(self, text_input):
        if any(word in text_input.lower() for word in ["triste", "depress", "chateado", "angry"]):
            return "üò¢ Parece que voc√™ est√° triste. Que tal um conte√∫do motivacional?"
        return "üôÇ Tudo certo!"

# ============================================
# Treinamento completo
# ============================================
def train_full(epochs=1, batch_size=1, show_progress=False):
    tokenizer = SimpleTokenizer()
    dataset = GPTMiniOmniDataset(tokenizer)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = GPTMini4Omni().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    scaler = torch.cuda.amp.GradScaler()

    for epoch in range(epochs):
        for batch_idx, (text_tokens, code_tokens, image_tensor, video_tensor, audio_tensor) in enumerate(dataloader):
            text_tokens, code_tokens = text_tokens.to(device), code_tokens.to(device)
            image_tensor, video_tensor = image_tensor.to(device), video_tensor.to(device)
            audio_tensor = audio_tensor.to(device)

            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                text_logits = model.forward_text(text_tokens)
                text_loss = criterion(text_logits.float(), torch.randn_like(text_logits))

                image_out = model.forward_image(image_tensor)
                image_loss = criterion(image_out, image_tensor)

                video_out = model.forward_video(video_tensor)
                video_loss = criterion(video_out, video_tensor)

                audio_out = model.forward_audio(audio_tensor)
                audio_loss = criterion(audio_out, torch.randn_like(audio_out))

                total_loss = text_loss + image_loss + video_loss + audio_loss

            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()

            if show_progress and batch_idx % 10 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {total_loss.item():.4f}")

    print("‚úÖ Treinamento finalizado!")
    return model

# ============================================
# Exemplo de uso
# ============================================
if __name__ == "__main__":
    model = train_full(epochs=1, batch_size=1, show_progress=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sample_image = torch.rand((3,1024,1024)).to(device)
    output_image = model.forward_image(sample_image)
    model.image_vae.show_image(output_image)

    msg = "Estou muito chateado hoje"
    print(model.analyze_user_sentiment(msg))
